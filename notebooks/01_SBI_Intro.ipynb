{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pranavm19/SBI-Tutorial/blob/main/notebooks/01_SBI_Intro.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install corner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, interact_manual, FloatSlider, IntSlider\n",
    "import corner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to Simulation Based Inference (SBI)\n",
    "**Pranav Mamidanna, PhD** (p.mamidanna22@imperial.ac.uk)\n",
    "\n",
    "In many scientific and engineering problems, we have access to powerful **simulators** that can generate data $\\mathbf{x}$ given parameters $\\theta$. \n",
    "\n",
    "Our goal is to do **Bayesian inference:** we specify a prior $p(\\theta)$ over the parameters, observe real data $\\mathbf{x}_{\\text{obs}}$, and want the posterior $p(\\theta \\mid \\mathbf{x}_{\\text{obs}})$.\n",
    "\n",
    "However, if the simulator does not yield a closed-form likelihood $p(\\mathbf{x} \\mid \\theta)$, standard Bayesian tools (like MCMC) become challenging or impossible. This is known as the **likelihood-free** or **simulation-based** setting.\n",
    "\n",
    "In this tutorial, we explore:\n",
    "1. How, even for a seemingly simple 2D “two-moons” simulator, deriving the exact likelihood is messy.  \n",
    "2. **Approximate Bayesian Computation (ABC)**, an approach that sidesteps the need for a closed-form likelihood by simulating data under proposed parameters and retaining those that generate data “close enough” to our observed dataset.  \n",
    "\n",
    "This notebook will walk through:\n",
    "- Defining the two-moons simulator,  \n",
    "- Illustrating the complexity of writing down $p(\\mathbf{x} \\mid \\theta)$,  \n",
    "- Implementing a basic ABC algorithm,  \n",
    "- Showcasing how the accepted parameter samples approximate the posterior.\n",
    "\n",
    "These ideas are a foundation for **Neural Simulation-Based Inference**, which trains neural networks to approximate posteriors or likelihoods when simulations are available but explicit probability formulas are not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Two Moons** _in the Posterior_\n",
    "\n",
    "Consider the following problem - we have a simulator $\\mathcal{Sim} : \\theta \\to x$, which computes $x$ as follows:\n",
    "\n",
    "$$a \\sim U\\left(-\\frac{\\pi}{2}, \\frac{\\pi}{2}\\right)$$\n",
    "\n",
    "$$r \\sim \\mathcal{N}(0.1,0.01^2)$$\n",
    "\n",
    "$$p = (r \\cos(a) + 0.25, r \\sin(a))$$\n",
    "\n",
    "$$x^\\top = p + \\left( \\frac{\\left| \\theta_1 + \\theta_2 \\right|}{\\sqrt{2}}, \\frac{-\\theta_1 + \\theta_2}{\\sqrt{2}} \\right)$$\n",
    "\n",
    "Which in code looks like - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_moons_sbi(theta, n_samples=100, sigma=0.01):\n",
    "    \"\"\"Generate a two moons posterior\"\"\"\n",
    "    alpha = np.random.uniform(-np.pi/2, np.pi/2, n_samples)\n",
    "    r = sigma * np.random.randn(n_samples) + 1\n",
    "    x_1 = r * np.cos(alpha) + 0.5 - np.abs(theta[0] + theta[1])/np.sqrt(2)\n",
    "    x_2 = r * np.sin(alpha) + (- theta[0] + theta[1])/np.sqrt(2)\n",
    "\n",
    "    x =  np.stack([x_1, x_2], axis=-1)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And generates data that looks like below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(theta1=0.5, theta2=-0.5):\n",
    "    theta = np.array([theta1, theta2])\n",
    "    x = two_moons_sbi(theta, n_samples=200)\n",
    "    \n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.scatter(x[:, 0], x[:, 1], s=5)\n",
    "    plt.xlim(-3, 3); plt.ylim(-3, 3)\n",
    "    plt.grid(True)\n",
    "\n",
    "# Create interactive sliders\n",
    "theta1_slider = FloatSlider(value=0.5, min=-2.0, max=2.0, step=0.1, description='theta1')\n",
    "theta2_slider = FloatSlider(value=-0.5, min=-2.0, max=2.0, step=0.1, description='theta2')\n",
    "\n",
    "interact(plot_data, theta1=theta1_slider, theta2=theta2_slider);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1. Where are the two moons? (Hint: how does the data look like when you simulate with $\\theta$ is $(\\theta_1, \\theta_2)$, and $(-\\theta_2, -\\theta_1)$ ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[4, 4])\n",
    "# plt.scatter(*two_moons_sbi(\"Set theta here\").T)\n",
    "# plt.scatter(*two_moons_sbi(\"Set theta here\").T)\n",
    "\n",
    "# Answer \n",
    "plt.scatter(*two_moons_sbi([0.5, 1.5]).T, alpha=0.2)\n",
    "plt.scatter(*two_moons_sbi([-1.5, -0.5]).T, alpha=0.2)\n",
    "\n",
    "plt.xlim(-3, 3); plt.ylim(-3, 3)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play around with the simulator below - vary theta and see how that affects the data distribution. \n",
    "\n",
    "You also see a red star - a single observation from the simulator, and by playing around with the theta you should (hopefully) uncover the two moons in the posterior :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll define a global list to hold our \"accepted\" parameter values\n",
    "accepted_params = []\n",
    "\n",
    "def update_plots(theta1=0.0, theta2=0.0):\n",
    "    \"\"\"\n",
    "    For the given (theta1, theta2):\n",
    "    1) Generate data and plot it in the data space.\n",
    "    2) Check if x_obs is within threshold distance of any point in that data.\n",
    "       If yes, record (theta1, theta2) in a global list.\n",
    "    3) Plot the \"accepted\" parameter list so far in the parameter space.\n",
    "    \"\"\"\n",
    "    global accepted_params\n",
    "\n",
    "    x_obs = np.array([0.0, 0.0])\n",
    "    threshold = 0.2   # acceptance threshold for \"close enough\"\n",
    "    col = 'C0' # Let's change color of samples when there is a match!\n",
    "\n",
    "    # Generate data from user specified theta\n",
    "    theta = np.array([theta1, theta2])\n",
    "    x_sim = two_moons_sbi(theta, n_samples=20)\n",
    "\n",
    "    # Check if any of the points in x_sim lie close enough to x_obs\n",
    "    # We'll measure Euclidean distance from each simulated point to x_obs\n",
    "    distances = np.linalg.norm(x_sim - x_obs, axis=1)\n",
    "    if np.any(distances < threshold):\n",
    "        accepted_params.append(theta)\n",
    "        accepted_params.append(np.array([-theta2, -theta1]))\n",
    "        col='C1'\n",
    "    else: \n",
    "        col='C0'\n",
    "        \n",
    "    # Plot the results\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "    # Data space\n",
    "    axes[0].scatter(*x_sim.T, color=col, alpha=0.6, label=\"Simulated data\")\n",
    "    \n",
    "    # Parameter space\n",
    "    axes[1].plot(theta1, theta2, 'rx', markersize=8, label=r\"Current $\\theta$\")\n",
    "    if len(accepted_params) > 0:\n",
    "        axes[1].scatter(*np.array(accepted_params).T, alpha=0.3, label=r\"$x_{obs} \\mid \\theta$\")\n",
    "\n",
    "    # Prettify\n",
    "    axes[0].set_title('Data space')\n",
    "    axes[0].plot(0, 0, 'r*', markersize=10, label=r\"$x_{obs} = [0,0]$\")\n",
    "    axes[0].set_xlim(-3, 3); axes[0].set_ylim(-3, 3); axes[0].grid(True)\n",
    "    axes[0].legend()\n",
    "    axes[1].set_title(\"Parameter Space\")\n",
    "    axes[1].set_xlabel(r\"$\\theta_1$\"); axes[1].set_ylabel(r\"$\\theta_2$\")\n",
    "    axes[1].set_xlim(-2.0, 2.0); axes[1].set_ylim(-2.0, 2.0)\n",
    "    axes[1].legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create sliders for theta1 and theta2\n",
    "theta1_slider = FloatSlider(value=0.5, min=-2.0, max=2.0, step=0.1, description='theta_1')\n",
    "theta2_slider = FloatSlider(value=0.5, min=-2.0, max=2.0, step=0.1, description='theta_2')\n",
    "\n",
    "interact(update_plots, theta1=theta1_slider, theta2=theta2_slider);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh, btw, you basically did simulation based inference :)\n",
    "\n",
    "But let's take a step back. Given we have an observation $x$ from this data generating process, how can we infer the parameters $\\theta$ that could have generated it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonus: Can we get an exact likelihood?\n",
    "\n",
    "Even in this simple “two-moons” simulator, the closed-form likelihood $p(\\mathbf{x}\\mid\\theta)$ is nontrivial to write down. Here’s the basic idea - our simulator draws:\n",
    "$$\n",
    "\\alpha \\sim \\mathcal{U}\\bigl(-\\tfrac{\\pi}{2}, \\tfrac{\\pi}{2}\\bigr), \n",
    "\\quad \n",
    "r \\sim \\mathcal{N}(0, \\sigma^2),\n",
    "$$\n",
    "and maps $(\\alpha, r)$ to a data point $x$ via:\n",
    "$$\n",
    "x_1 = r \\cos(\\alpha) + 0.25 - \\dfrac{|\\theta_1 + \\theta_2|}{\\sqrt{2}}, \n",
    "\\quad\n",
    "x_2 = r \\sin(\\alpha) + \\dfrac{\\theta_2 - \\theta_1}{\\sqrt{2}}.\n",
    "$$\n",
    "\n",
    "This means that we can use the **change of variables formula** to go from $(\\alpha, r) \\to x$. That is, we can write the density of $x$ as\n",
    "$$\n",
    "p(x \\mid \\theta) \\propto ~ p(\\alpha) ~ p(r) ~ \\bigl|\\dfrac{d(\\alpha, r)}{d(x_1, x_2)}\\bigr|\n",
    "$$\n",
    "\n",
    "> To-do: derive a simplified expression for the likelihood.\n",
    "\n",
    "In larger or more realistic simulators (with more dimensions, complicated physics, etc.), deriving such an expression by hand may be **extremely** difficult or effectively impossible. **This** is why we turn to likelihood-free methods—like **Approximate Bayesian Computation** or **Neural Simulation-Based Inference**—which avoid having to evaluate $p(\\mathbf{x}\\mid\\theta)$ directly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Approximate Bayesian Computation (ABC)**\n",
    "\n",
    "Approximate Bayesian Computation (ABC) is a class of inference methods designed for settings where the likelihood function $p(x \\mid \\theta)$ is *intractable*—but we can still **simulate** from it. The basic idea is this -- simulators let us verify _whether_ a particular $\\theta$ could have generated an observed $x_{obs}$! If we simulate _A LOT_ of $\\theta$'s, then we can use the sampled $x$'s to _infer_ the posterior distribution!\n",
    "\n",
    "**Ingredients list**\n",
    "\n",
    "- A prior distribution $p(\\theta)$: We can sample from this to generate candidate parameter values.\n",
    "- A simulator: Given $\\theta$, we simulate synthetic data $\\mathbf{x}_{\\text{sim}} \\sim p(\\mathbf{x} \\mid \\theta)$.\n",
    "- A distance function $d(\\mathbf{x}_{\\text{sim}}, \\mathbf{x}_{\\text{obs}})$: Measures how close the simulated data is to the observed data.\n",
    "- A threshold $\\varepsilon$: Determines how \"close\" is close enough.\n",
    "\n",
    "**The Basic ABC Algorithm** \n",
    "\n",
    "Repeat until you have $N$ accepted samples:\n",
    "1. Sample $\\theta^* \\sim p(\\theta)$ from the prior.\n",
    "2. Simulate $\\mathbf{x}_{\\text{sim}} \\sim p(\\mathbf{x} \\mid \\theta^*)$ using the simulator.\n",
    "3. If $d(\\mathbf{x}_{\\text{sim}}, \\mathbf{x}_{\\text{obs}}) < \\varepsilon$, accept $\\theta^*$ as a posterior sample.\n",
    "\n",
    "The collection of accepted $\\theta$ values approximates the posterior $p(\\theta \\mid \\mathbf{x}_{\\text{obs}})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2. Based on the algorithm above, fill in the blanks in the following code to run your own ABC inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abc(x_obs, eps=0.1, n_samples=1000):\n",
    "    \"\"\"ABC algorithm for two moons model.\n",
    "\n",
    "    Args:\n",
    "        x_obs (np.ndarray): Observation from the two moons model.\n",
    "        eps (float, optional): Acceptance threshold. Defaults to 0.01.\n",
    "        n_samples (int, optional): Number of samples after which to stop. Defaults to 1000.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Accepted samples approximating the posterior p(theta|x_obs).\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    accept_flag = []\n",
    "    n_accepted = sum(accept_flag)\n",
    "    n_sim = 100 # samples to simulate each time\n",
    "\n",
    "    # Make a progress bar to see how quickly we accept\n",
    "    pbar = tqdm(total=n_samples, desc=\"Accepted Samples\", unit=\"samples\")\n",
    "\n",
    "    while sum(accept_flag) < n_samples:\n",
    "        # 1. Sample a parameter from the prior: Unif(-2, 2)\n",
    "        # theta_candidate = # FILL IN THE BLANK\n",
    "        theta_candidate = np.random.uniform(low=-2, high=2, size=2)\n",
    "        samples.append(theta_candidate) # bookkeeping number of theta candidates evaluated\n",
    "\n",
    "        # 2. Simulate data from that parameter\n",
    "        x_sim = two_moons_sbi(theta_candidate, n_samples=n_sim)\n",
    "\n",
    "        # 3. Compute the distance, reject if outside threshold\n",
    "        # dist = # FILL IN THE BLANK\n",
    "        dist = np.linalg.norm(x_sim - x_obs, axis=1) \n",
    "\n",
    "        if np.any(dist < eps):\n",
    "            accept_flag.append(1) # bookkeeping accepted thetas\n",
    "            n_accepted += np.sum(dist < eps)\n",
    "            acceptance_ratio = n_accepted / (len(samples) * n_sim)\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix_str(f\"acceptance_ratio={acceptance_ratio:.3f}\")\n",
    "        else:\n",
    "            accept_flag.append(0)\n",
    "\n",
    "    pbar.close()\n",
    "    return np.array(samples), np.array(accept_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify this runs without any errors!\n",
    "candidates, flags = abc([0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see ABC in action! Can it compete with how you explored the parameter space previously?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "global_accepted_samples = []\n",
    "\n",
    "def run_abc(n_samples=100, eps=0.1):\n",
    "    \"\"\"\n",
    "    Each time run_interact is pressed:\n",
    "    1. Run ABC to get n_samples new samples.\n",
    "    2. Display them one by one on the plot, maintaining history of all previous runs.\n",
    "    \"\"\"\n",
    "    global global_accepted_samples\n",
    "    \n",
    "    # Run ABC with a single observed point [0,0]\n",
    "    samples, accept_flag = abc(np.array([0.0, 0.0]), eps, n_samples)\n",
    "    \n",
    "    # Show new samples one-by-one\n",
    "    for i, sample in enumerate(samples):\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "    \n",
    "        # First plot all previously accepted samples\n",
    "        if global_accepted_samples:\n",
    "            ax.scatter(*np.array(global_accepted_samples).T, alpha=0.6, label='Previous samples')\n",
    "    \n",
    "        # Plot current sample\n",
    "        ax.plot(sample[0], sample[1], 'rx', markersize=10, label='Current sample')\n",
    "        \n",
    "        # If accepted, add to plot and global list\n",
    "        if accept_flag[i] == 1:\n",
    "            global_accepted_samples.append(sample)\n",
    "            ax.scatter(sample[0], sample[1], alpha=0.6)\n",
    "        \n",
    "        # Prettify\n",
    "        ax.set_xlim(-2, 2); ax.set_ylim(-2, 2)\n",
    "        ax.set_xlabel(r'$\\theta_1$'); ax.set_ylabel(r'$\\theta_2$')\n",
    "        ax.set_title(f\"ABC Posterior Samples\")\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        plt.show()\n",
    "        # plt.pause(0.005)  # Small pause for visualization\n",
    "        \n",
    "samples_slider = IntSlider(value=50, min=10, max=200, step=10, description='Number of samples to acquire')\n",
    "eps_slider = FloatSlider(value=0.2, min=0.1, max=1, step=0.1, description='Acceptance threshold')\n",
    "\n",
    "interact_manual(run_abc, n_samples=samples_slider, eps=eps_slider);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the posterior distribution using corner\n",
    "samples, accept_flag = abc([0, 0])\n",
    "figure = corner.corner(samples[accept_flag==1], labels=['theta1', 'theta2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok, we have posterior samples, now what??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discuss in groups\n",
    "1. How does the inference (the approximate posterior distribution) depend on \n",
    "   the number of samples n_samples in ABC?\n",
    "2. How does changing eps_thresh (epsilon threshold) affect the posterior?\n",
    "3. Plot some additional examples of observed data for different 'theta_true'.\n",
    "4. (Advanced) Instead of using the naive distance, experiment with different summary statistics or distance metrics. For example, compare the distribution of means, or the distribution of pairwise distances among points, etc. This is especially important when x_obs is high-dimensional.\n",
    "5. (Advanced) Here, we have performed inference for a single observation. What happens when we have multiple $x_{obs}$? Do you think ABC scales well?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outro\n",
    "\n",
    "While ABC is very simple and elegant, it can be quite inefficient in high dimensions, or when choosing the right distance metric or threshold is hard. These limitations motivate more scalable approaches like **Neural Simulation-Based Inference (Neural SBI)**, which we will touch on later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note on \"computational intractability\"**\n",
    "\n",
    "While we have seen above that writing down an expression for the likelihood of two moons is ugly, but not impossible - i.e., it is still computationally tractable. But in several real-world applications, it is actually intractable to compute the likelihood. \n",
    "#### Additional Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
