{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pranavm19/SBI-Tutorial/blob/main/notebooks/02_NFlows.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install sbi corner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall the two moons model\n",
    "def two_moons_sbi(theta, sigma=0.01):\n",
    "    \"\"\"Generate a two moons posterior\"\"\"\n",
    "    n_samples = theta.shape[0]\n",
    "    alpha = np.random.uniform(-np.pi/2, np.pi/2, n_samples)\n",
    "    r = sigma * np.random.randn(n_samples) + 1\n",
    "    x_1 = r * np.cos(alpha) + 1 - np.abs(theta[:, 0] + theta[:, 1])/np.sqrt(2)\n",
    "    x_2 = r * np.sin(alpha) + (- theta[:, 0] + theta[:, 1])/np.sqrt(2)\n",
    "\n",
    "    x =  np.stack([x_1, x_2], axis=-1)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outline\n",
    "\n",
    "0. Intro (why neural SBI, inference and density estimation with neural networks)\n",
    "1. What are normalizing flow, and how do they work (change of variables formula + intuition building using udl textbook Fig. 16.2)\n",
    "2. The I/O of normalizing flows - what do we want them to do?\n",
    "2. Build an affine coupling layer (Real NVP equations, introducing context into the equations, implementing AffineCouplingLayer, NormalizingFlow)\n",
    "3. Normalizing flows using affine coupling layers and permutation masks (train on two moons model)\n",
    "4. Reproduce using SBI package\n",
    "4. Performing inference (corner plots, predictive checks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **What are normalizing flows?**\n",
    "\n",
    "#### Inference and density estimation\n",
    "\n",
    "\n",
    "#### Change of variables formula\n",
    "\n",
    "\n",
    "#### Transforming a data distribution to a base density\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base density, transform, and data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make figure 16.2 from the udl book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The API\n",
    "\n",
    "Cool, so we have seen how a bijective transform can be used to \"normalize\" a given density. Let's make them a bit more powerful ðŸ’ª\n",
    "\n",
    "There are two components to a normalizing flow : the bijective transform (or a set of them), and the prior distribution. Once we have these, at train time, we can compute z given a batch of (x, context), and evaluate the loss. At test time, we can draw a sample from (z, context), and generate samples from the learned data distribution x (and evaluate the probability density!!). To do this, we will need:\n",
    "\n",
    "`NF = NormalizingFlow(flows, prior)`\n",
    "- `NF.forward(x, context) -> z, ldj`, \n",
    "- `NF.sample(z, context) -> x, ldj`\n",
    "\n",
    "The `flows` object is usually a list of layers each of which is a bijective transform:\n",
    "\n",
    "`T = FlowTransform()`\n",
    "- `T.forward(x, context) -> z, ldj`\n",
    "- `T.inverse(z, context) -> x, ldj`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Why are they suitable for Simulation-Based Inference?**\n",
    "\n",
    "Here, we will specifically deal with the case of Neural Posterior Estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Real NVP - Affine Coupling Flows**\n",
    "\n",
    "A popular implementation of normalizing flows is **Real NVP** (Dinh et al., 2017). The core of Real NVP is the **affine coupling** transformation. \n",
    "\n",
    "Suppose we split the data $x$ of dimension $D$ into two parts, $x = [x_{1:d},\\, x_{(d+1):D}]$. In a **coupling layer**, we leave one part unchanged and apply a learnable affine transformation to the other part. Specifically, let:\n",
    "\n",
    "$$\n",
    "    y_{1:d} \\;=\\; x_{1:d},\n",
    "$$\n",
    "$$\n",
    "    y_{(d+1):D} \\;=\\; x_{(d+1):D}\\,\\odot \\exp\\bigl(s_\\theta(x_{1:d})\\bigr)\\;+\\; t_\\theta(x_{1:d}),\n",
    "$$\n",
    "\n",
    "where $\\odot$ denotes elementwise multiplication. The functions $s_\\theta(\\cdot)$ and $t_\\theta(\\cdot)$ (the \"scale\" and \"shift\" networks) are typically small neural networks that depend on the \"frozen\" part $x_{1:d}$.\n",
    "\n",
    "- **Invertibility**: This transformation is **invertible** because you can solve for $x_{(d+1):D}$ by reversing the shift and scale operations:\n",
    "  $$\n",
    "    x_{(d+1):D} \n",
    "     = \\Bigl(y_{(d+1):D} - t_\\theta(y_{1:d})\\Bigr)\\,\\odot \\exp\\Bigl(- s_\\theta(y_{1:d})\\Bigr).\n",
    "  $$\n",
    "  The log-determinant of the Jacobian $\\left\\lvert \\det \\frac{\\partial y}{\\partial x} \\right\\rvert$ is simply\n",
    "  $$\n",
    "    \\sum_{j=1}^{D-d} s_\\theta(x_{1:d})_j,\n",
    "  $$\n",
    "  because the scaling is diagonal in the sub-block.\n",
    "\n",
    "- **Why is it non-linear?** \n",
    "  Although the transformation is written as an *affine* function for the second block, **the parameters of that affine transformation are themselves neural-network outputs**, i.e., $s_\\theta(\\cdot)$ and $t_\\theta(\\cdot)$. This makes the overall mapping\n",
    "  $$\n",
    "    x \\mapsto y\n",
    "  $$\n",
    "  **non-linear** in $x$. The \"frozen\" part $x_{1:d}$ is feeding through a neural network to produce scale and shift factors, which can be highly non-linear functions of $x_{1:d}$. \n",
    "\n",
    "- **Coupling layers and permutations**: In Real NVP, we often interleave such coupling layers with **permutation** layers to ensure that over multiple layers, each dimension eventually appears in the \"frozen\" part and the \"transformed\" part. This broadens the flexibility of the flow, letting it model complex dependencies across all dimensions.\n",
    "\n",
    "In summary, **Real NVP** is a straightforward yet powerful example of how normalizing flows combine tractable Jacobians (via affine coupling) with flexible function approximators (neural networks for scale and shift). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1. We have seen above that for NPE, we need to train a conditional normalizing flow, where $x$ are the conditioning variables (also called as context), and $\\theta$ are primary variables what get normalized. However, in the above equations, we don't see any conditioning variables. Can you modify the forward and backward equations such that show how context is utilized?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2. With all the equations to implement a conditional normalizing flow at hand, let's implement it! Complete the AffineCouplingLayer class below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AffineCouplingLayer(nn.Module):\n",
    "    def __init__(self, input_dim, context_dim):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.context_dim = context_dim\n",
    "        self.split_idx = input_dim - (input_dim // 2) # first part gets more dims if input_dim is odd\n",
    "\n",
    "        # Define scale and shift networks\n",
    "        self.scale_net = nn.Sequential(\n",
    "            nn.Linear(self.split_idx + context_dim, 64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(64, 64), \n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(64, input_dim - self.split_idx)\n",
    "        )\n",
    "        self.shift_net = nn.Sequential(\n",
    "            nn.Linear(self.split_idx + context_dim, 64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(64, 64), \n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(64, input_dim - self.split_idx)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, context):\n",
    "        # Split input tensor along the last dimension\n",
    "        x_identity = x[..., :self.split_idx]\n",
    "        x_transform = x[..., self.split_idx:]\n",
    "\n",
    "        # Concatenate identity and context for both networks\n",
    "        # identity_context = # FILL IN THE BLANK\n",
    "        # scale = # FILL IN THE BLANK\n",
    "        # shift = # FILL IN THE BLANK\n",
    "\n",
    "        identity_context = torch.cat((x_identity, context), dim=-1)\n",
    "        scale = self.scale_net(identity_context)\n",
    "        shift = self.shift_net(identity_context)\n",
    "        \n",
    "        # Compute log-determinant of the Jacobian\n",
    "        # ldj = # FILL IN THE BLANK\n",
    "        ldj = torch.sum(scale, dim=-1)\n",
    "\n",
    "        # Affine transformation on x_transform\n",
    "        # z_transform = # FILL IN THE BLANK\n",
    "        \n",
    "        z_transform = x_transform * torch.exp(scale) + shift\n",
    "\n",
    "        # Concatenate unchanged part with transformed part\n",
    "        z = torch.cat((x_identity, z_transform), dim=-1)\n",
    "        return z, ldj\n",
    "    \n",
    "    def inverse(self, z, context):\n",
    "        # Inverse transform: split z into identity and transformed parts\n",
    "        z_identity = z[..., :self.split_idx]\n",
    "        z_transform = z[..., self.split_idx:]\n",
    "\n",
    "        # Concatenate identity and context, pass them into the networks\n",
    "        identity_context = \n",
    "        scale = \n",
    "        shift = \n",
    "\n",
    "        identity_context = torch.cat((z_identity, context), dim=-1)\n",
    "        scale = self.scale_net(identity_context)\n",
    "        shift = self.shift_net(identity_context)\n",
    "        \n",
    "        # Compute log-determinant of the Jacobian\n",
    "        ldj = # FILL IN THE BLANK\n",
    "        ldj = -torch.sum(scale, dim=-1)\n",
    "\n",
    "        # Inverse affine transformation\n",
    "        # x_transform = # FILL IN THE BLANK\n",
    "        x_transform = (z_transform - shift) * torch.exp(-scale)\n",
    "\n",
    "        # Concatenate identity and transformed parts\n",
    "        x = torch.cat((z_identity, x_transform), dim=-1)\n",
    "        return x, ldj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizingFlow(nn.Module):\n",
    "    \"\"\"\n",
    "    A normalizing flow model composed of a sequence of affine coupling layers and a prior distribution.\n",
    "    \"\"\"\n",
    "    def __init__(self, flows, prior=None):\n",
    "        super().__init__()\n",
    "        self.flows = nn.ModuleList(flows)\n",
    "        self.dim = self.flows[0].input_dim\n",
    "        # Initialize the prior distribution (device will be set correctly later)\n",
    "        if prior is None:\n",
    "            self.prior = torch.distributions.MultivariateNormal(\n",
    "                torch.zeros(self.dim), torch.eye(self.dim))\n",
    "        else:\n",
    "            self.prior = prior\n",
    "\n",
    "        self.train_loss = []\n",
    "\n",
    "    def forward(self, x, context):\n",
    "        \"\"\"\n",
    "        Applies a sequence of flow transformations and accumulates the log-determinants.\n",
    "        \"\"\"\n",
    "        ldj = torch.zeros(x.shape[0], device=x.device)\n",
    "        for flow in self.flows:\n",
    "            x, ldj_ = flow(x, context)\n",
    "            ldj += ldj_\n",
    "        return x, ldj\n",
    "\n",
    "    def inverse(self, z, context):\n",
    "        \"\"\"\n",
    "        Inverts the flow transformation from latent space back to the input space.\n",
    "        \"\"\"\n",
    "        ldj = torch.zeros(z.shape[0], device=z.device)\n",
    "        for flow in reversed(self.flows):\n",
    "            z, ldj_ = flow.inverse(z, context)\n",
    "            ldj += ldj_  # log-determinants are already negated in inverse\n",
    "        return z, ldj\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, num_samples, context):\n",
    "        \"\"\"\n",
    "        Generate samples from the model given a context.\n",
    "        \"\"\"\n",
    "        device = next(self.parameters()).device\n",
    "        z = self.prior.sample((num_samples,)).to(device)\n",
    "        x, _ = self.inverse(z, context)\n",
    "        return x\n",
    "\n",
    "    def log_prob(self, x, context): \n",
    "        \"\"\"\n",
    "        Compute the log probability of x under the flow model.\n",
    "        \"\"\"\n",
    "        z, ldj = self(x, context)\n",
    "        log_pz = self.prior.log_prob(z)\n",
    "        return log_pz + ldj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PermutationLayer(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        # Create a random permutation for the feature indices.\n",
    "        perm = torch.randperm(num_features)\n",
    "        self.register_buffer(\"perm\", perm)\n",
    "        self.register_buffer(\"inv_perm\", torch.argsort(perm))\n",
    "    \n",
    "    def forward(self, x, context):\n",
    "        # Permuting the features; no effect on the log-determinant.\n",
    "        x_permuted = x[..., self.perm]\n",
    "        # Log-determinant is zero for a permutation\n",
    "        log_det = torch.zeros(x.size(0), device=x.device)\n",
    "        return x_permuted, log_det\n",
    "    \n",
    "    def inverse(self, x, context):\n",
    "        # Inverse permutation\n",
    "        x_inv = x[..., self.inv_perm]\n",
    "        log_det = torch.zeros(x.size(0), device=x.device)\n",
    "        return x_inv, log_det\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle dimensions\n",
    "input_dim = 2\n",
    "context_dim = 2\n",
    "n_layers = 4\n",
    "flows = []\n",
    "\n",
    "# Define the model and optimizer\n",
    "for i in range(n_layers):\n",
    "    flows.append(AffineCouplingLayer(input_dim, context_dim))\n",
    "    flows.append(PermutationLayer(input_dim))\n",
    "\n",
    "model = NormalizingFlow(flows)\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix an observed data point x_obs\n",
    "x_obs = torch.tensor([[0.0, 0.0]], dtype=torch.float32)\n",
    "\n",
    "# Create a grid of theta-values over which to evaluate the posterior\n",
    "n_samples = 100\n",
    "theta0_vals = torch.linspace(-2, 2, n_samples)\n",
    "theta1_vals = torch.linspace(-2, 2, n_samples)\n",
    "TH0, TH1 = torch.meshgrid(theta0_vals, theta1_vals, indexing='xy')\n",
    "theta_grid = torch.cat([TH0.reshape(-1,1), TH1.reshape(-1,1)], dim=1)\n",
    "\n",
    "# Training settings\n",
    "num_iter = 5000\n",
    "num_update_iter = 100\n",
    "batch_size = 256\n",
    "losses = []\n",
    "\n",
    "# Prepare figure\n",
    "fig, (ax_loss, ax_posterior) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "plt.ion()\n",
    "\n",
    "for i in range(num_iter):\n",
    "    # Generate data\n",
    "    theta = np.random.uniform(-4, 4, size=(batch_size, 2))\n",
    "    x = two_moons_sbi(theta)\n",
    "\n",
    "    # Standard training step\n",
    "    x = torch.tensor(x, dtype=torch.float32)\n",
    "    theta = torch.tensor(theta, dtype=torch.float32)\n",
    "    optimizer.zero_grad()\n",
    "    loss = -model.log_prob(x, theta).mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    # Update plots interactively\n",
    "    if i % num_update_iter == 0:\n",
    "        ax_loss.cla()\n",
    "        ax_posterior.cla()\n",
    "\n",
    "        # Training loss \n",
    "        ax_loss.plot(losses, label='Train Loss')\n",
    "        ax_loss.set_title('Training Loss')\n",
    "        ax_loss.set_xlabel('Iteration')\n",
    "        ax_loss.set_ylabel('Negative Log-Likelihood')\n",
    "        ax_loss.legend()\n",
    "\n",
    "        # Approximate posterior \n",
    "        with torch.no_grad():\n",
    "            # Replicate x_obs for every point in theta_grid so the shape matches\n",
    "            x_obs_tiled = x_obs.repeat(theta_grid.shape[0], 1)\n",
    "\n",
    "            # Posterior ~ exp(log p(x_obs | theta))\n",
    "            post_vals = model.log_prob(x_obs_tiled, theta_grid).exp()\n",
    "            post_2d = post_vals.view(n_samples, n_samples)\n",
    "\n",
    "        # Contour-plot the posterior in theta-space\n",
    "        c = ax_posterior.contourf(\n",
    "            TH0.numpy(), TH1.numpy(), post_2d.numpy(),\n",
    "            levels=50, alpha=0.8\n",
    "        )\n",
    "        ax_posterior.set_title(f'Posterior at iteration {i}')\n",
    "        ax_posterior.set_xlabel(r'$\\theta_0$')\n",
    "        ax_posterior.set_ylabel(r'$\\theta_1$')\n",
    "        \n",
    "        # Optionally add a colorbar if you like\n",
    "        # fig.colorbar(c, ax=ax_posterior)\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        display(fig)\n",
    "\n",
    "plt.ioff();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The really cool thing is that we don't need to re-run the algorithm\n",
    "# to infer posterior for a new data point! (more or less...)\n",
    "x_obs = torch.tensor([[0.1, 0.1]], dtype=torch.float32)\n",
    "\n",
    "# Create a grid of theta-values over which to evaluate the posterior\n",
    "n_samples = 100\n",
    "theta0_vals = torch.linspace(-3, 3, n_samples)\n",
    "theta1_vals = torch.linspace(-3, 3, n_samples)\n",
    "TH0, TH1 = torch.meshgrid(theta0_vals, theta1_vals, indexing='xy')\n",
    "theta_grid = torch.cat([TH0.reshape(-1,1), TH1.reshape(-1,1)], dim=1)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=[4, 4])\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Replicate x_obs for every point in theta_grid so the shape matches\n",
    "    x_obs_tiled = x_obs.repeat(theta_grid.shape[0], 1)\n",
    "\n",
    "    # Posterior ~ exp(log p(x_obs | theta))\n",
    "    post_vals = model.log_prob(x_obs_tiled, theta_grid).exp()\n",
    "    post_2d = post_vals.view(n_samples, n_samples)\n",
    "\n",
    "# Contour-plot the posterior in theta-space\n",
    "c = ax.contourf(\n",
    "    TH0.numpy(), TH1.numpy(), post_2d.numpy(),\n",
    "    levels=50, alpha=0.8\n",
    ")\n",
    "ax.set_title(f'Posterior at iteration {i}')\n",
    "ax.set_xlabel(r'$\\theta_0$')\n",
    "ax.set_ylabel(r'$\\theta_1$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot posterior using corner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Repeat using `sbi` toolbox**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use sbi for the same analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Posterior checks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Outro**\n",
    "\n",
    "#### Additional reading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
